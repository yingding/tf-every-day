{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP 101 - explaining ml models and beyond\n",
    "\n",
    "### Feature Attributions\n",
    "* SHAP (SHapley Additive exPlanations) - https://shap.readthedocs.io/en/latest/\n",
    "* Understand individual predictions - https://www.kaggle.com/code/dansbecker/shap-values/tutorial\n",
    "* Aggregate SHAP values for even more detailed model insights - https://www.kaggle.com/code/dansbecker/advanced-uses-of-shap-values/tutorial\n",
    "* Style SHAP Plots - https://towardsdatascience.com/how-to-easily-customize-shap-plots-in-python-fdff9c0483f2?gi=754319eae2c8\n",
    "\n",
    "* Convert SHAP Score to percentage - https://medium.com/towards-data-science/* black-box-models-are-actually-more-explainable-than-a-logistic-regression-f263c22795d\n",
    "* base value in SHAP (average of the outcome variable in the training set) - https://towardsdatascience.com/explainable-ai-xai-with-shap-multi-class-classification-problem-64dd30f97cea\n",
    "* Additive Feature importance - https://medium.com/@singhvis929/additive-feature-attribution-methods-diving-into-ml-explainability-98c81c656d3d\n",
    "* Kernel in SHAP - https://towardsdatascience.com/one-feature-attribution-method-to-supposedly-rule-them-all-shapley-values-f3e04534983d\n",
    "\n",
    "### Partial Dependence Plot\n",
    "* Partial Dependence Plot Theory - https://christophm.github.io/interpretable-ml-book/pdp.html\n",
    "* Partial Dependence Plots - https://scikit-learn.org/stable/modules/partial_dependence.html\n",
    "* Partial Dependence Plots show how a feature affects predictions - https://www.kaggle.com/code/dansbecker/partial-plots\n",
    "\n",
    "### Additional References\n",
    "* Fairlearn - https://fairlearn.org\n",
    "* squaredev.io - https://github.com/squaredev-io/explainable-ai\n",
    "* Rerun-sdk - https://pypi.org/project/rerun-sdk/\n",
    "\n",
    "### Pandas \n",
    "* Working with missing value - https://pandas.pydata.org/docs/user_guide/missing_data.html\n",
    "* show max columns - https://stackoverflow.com/questions/11707586/how-do-i-expand-the-output-display-to-see-more-columns-of-a-pandas-dataframe/11711637#11711637\n",
    "* DataFrame types with dtypes - https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dtypes.html\n",
    "\n",
    "### Scikit-Learn\n",
    "* Custom Demo Classifier _ https://scikit-learn.org/stable/developers/develop.html\n",
    "* Custom Classification Model - https://towardsdatascience.com/how-to-create-custom-scikit-learn-classification-and-regression-models-70db7e76addd\n",
    "* Custom ensemble Model - https://towardsdatascience.com/how-to-build-a-custom-estimator-for-scikit-learn-fddc0cb9e16e?gi=b6f190dcd368\n",
    "* Custom Regression Model - https://towardsdatascience.com/building-a-custom-model-in-scikit-learn-b0da965a1299\n",
    "\n",
    "### Model validation\n",
    "Confusion Matrix, ROC, AUC - https://towardsdatascience.com/intuition-behind-roc-auc-score-1456439d1f30?gi=c4b96aa0c60e\n",
    "\n",
    "### Explainable AI\n",
    "* The Challenge of Crafting Intelligible Intelligence - https://cacm.acm.org/magazines/2019/6/237004-the-challenge-of-crafting-intelligible-intelligence/fulltext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Titanic Compitition\n",
    "https://www.kaggle.com/competitions/titanic\n",
    "\n",
    "### Data Description\n",
    "https://www.kaggle.com/competitions/titanic/data?select=train.csv\n",
    "\n",
    "| Variable | Definition\t| Key | \n",
    "| :--- | :--- | :--- |\n",
    "| survival | Survival |\t0 = No, 1 = Yes |\n",
    "| pclass   | Ticket class |\t1 = 1st, 2 = 2nd, 3 = 3rd |\n",
    "| sex |\tSex\t| |\n",
    "| Age |\tAge | in years | \t\n",
    "| sibsp\t| # of siblings / spouses <br>aboard the Titanic |\t\n",
    "| parch\t| # of parents / children <br>aboard the Titanic |\t\n",
    "| ticket |\tTicket number | |\t\n",
    "| fare | Passenger fare | (Ticket price paid)  |\t\n",
    "| cabin\t| Cabin number | |\t\n",
    "| embarked\t| Port of Embarkation |\tC = Cherbourg, <br>Q = Queenstown, <br>S = Southampton |\n",
    "\n",
    "### Variable Notes\n",
    "\n",
    "**pclass:** A proxy for socio-economic status (SES)\n",
    "* 1st = Upper\n",
    "* 2nd = Middle\n",
    "* 3rd = Lower\n",
    "\n",
    "**age:** Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n",
    "\n",
    "**sibsp:** The dataset defines family relations in this way...\n",
    "* Sibling = brother, sister, stepbrother, stepsister\n",
    "* Spouse = husband, wife (mistresses and fianc√©s were ignored)\n",
    "\n",
    "**parch:** The dataset defines family relations in this way...\n",
    "* Parent = mother, father\n",
    "* Child = daughter, son, stepdaughter, stepson\n",
    "\n",
    "Some children travelled only with a nanny, therefore parch=0 for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# load JS visualization code to notebook\n",
    "import shap\n",
    "# used for render kernelshap explainer inference progressbar \n",
    "shap.initjs()\n",
    "\n",
    "# graphic display modes\n",
    "%matplotlib inline\n",
    "\n",
    "# DARK_MODE = True\n",
    "DARK_MODE = False\n",
    "\n",
    "\"\"\"the custom argo presented in this notebook is not suitable for kernel explainer\"\"\"\n",
    "# ALGO=\"custom\"\n",
    "# KERNEL_EXPLAINER = False\n",
    "\n",
    "ALGO=\"xgboost\"\n",
    "KERNEL_EXPLAINER = True\n",
    "# KERNEL_EXPLAINER = False\n",
    "\n",
    "# LINK = \"logit\" # to transfer the shap value from probability space to log(odds) space to have value evenly distributed.\n",
    "LINK = \"identity\" # to get the shap value of probability space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datahelper import (\n",
    "    KaggleData, \n",
    "    current_dir_subpath,\n",
    "    profile, \n",
    "    feature_correlation, \n",
    "    na_columns,\n",
    "    fill_missing_values_with_mean,\n",
    "    DataVisualizer,\n",
    ")\n",
    "\n",
    "from utils.modelhelper import (\n",
    "    ProbBinaryClassifier,\n",
    "    ModelValidator,\n",
    "    ModelExplainer,\n",
    "    ModelKernelExplainer\n",
    ")\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# set the pandas display\n",
    "pd.set_option('display.max_columns', 15)\n",
    "pd.set_option('display.width', 1000) # for print\n",
    "\n",
    "# create a visualization helper object\n",
    "visual_helper = DataVisualizer(dark_mode=DARK_MODE)\n",
    "\n",
    "titanic_train_path = current_dir_subpath(\"data/train.csv\")\n",
    "titanic_test_path = current_dir_subpath(\"data/test.csv\")\n",
    "label_name = \"Survived\"\n",
    "one_hot_cols = [\"Sex\"]\n",
    "\n",
    "titanic = KaggleData(\n",
    "     train_path = titanic_train_path,\n",
    "     test_path = titanic_test_path,\n",
    "     label_col=label_name\n",
    ")\n",
    "\n",
    "# load all raw unprocessed data as DataFrame, label as Series\n",
    "# one_hot_cols transfers categorical column to one_hot encoded column \n",
    "train_X_raw_df, test_X_raw_df, train_raw_y = titanic.load(one_hot_cols=one_hot_cols)\n",
    "all_X_raw_df = titanic.load_all(one_hot_cols=one_hot_cols)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore numerical features in different data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect the numerical features for building classifier\n",
    "num_features = all_X_raw_df.describe().columns.to_list()\n",
    "print(num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile(train_X_raw_df, title=\"Profile of Raw Training Dataset\")\n",
    "print(\"\\n\" + \"#\" * 20)\n",
    "profile(test_X_raw_df, title=\"Profile of Raw Test Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# titanic.boxplot_dist([\"PassengerId\", \"Fare\", \"Age\"], marker=\"x\", orient=\"h\", legend=\"upper right\", dark_mode=DARK_MODE)\n",
    "titanic.boxplot_dist([\"Fare\", \"Age\"], marker=\"x\", orient=\"h\", legend=\"upper right\", dark_mode=DARK_MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.boxplot_dist(['Pclass', 'SibSp', 'Parch'], marker=\"x\", orient=\"h\", legend=\"upper right\", dark_mode=DARK_MODE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inputation\n",
    "\n",
    "replace the NaN values of numerical features in training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X_raw_df.describe()\n",
    "# test_X_raw_df.describe()\n",
    "# all_X_raw_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_values(x):\n",
    "    \"\"\"x is tuple with two positions\"\"\"\n",
    "    match x[0]:\n",
    "        case \"Age\":\n",
    "            return (x[0], round(x[1]))\n",
    "        case \"Fare\":\n",
    "            return (x[0], round(x[1], 4)) # returns float with rounded 4 decimals\n",
    "        case _:\n",
    "            return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_raw_df, train_mean_dict = fill_missing_values_with_mean(\n",
    "    df=train_X_raw_df, pop_df=all_X_raw_df, filter_cols=num_features, filter_func=filter_values)\n",
    "print(train_mean_dict)\n",
    "print(f\"no. of numerical cols. has NaN values: {len(na_columns(train_X_raw_df, num_features))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_raw_df, test_mean_dict = fill_missing_values_with_mean(\n",
    "    df=test_X_raw_df, pop_df=all_X_raw_df, filter_cols=num_features, filter_func=filter_values)\n",
    "print(test_mean_dict)\n",
    "print(f\"no. of numerical cols. has NaN values: {len(na_columns(test_X_raw_df, num_features))}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the correlation of numerical features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_y_raw_df = pd.concat([train_X_raw_df, train_raw_y], axis=1)\n",
    "threshold = 0.2\n",
    "corr_df, high_corr_df = feature_correlation(train_X_y_raw_df, label=label_name, threshold=threshold)\n",
    "corr_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_helper.display_feature_correlation(corr_df=corr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the features with high correlation to the label\n",
    "high_corr_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only numerical features\n",
    "selected_features = num_features.copy()\n",
    "selected_features.remove(\"PassengerId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the training data\n",
    "X_train, X_valid, y_train, y_valid = titanic.split(\n",
    "    titanic.select_cols(df=train_X_raw_df, cols=selected_features), \n",
    "    train_raw_y, test_size=0.2, random_state=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algo factory for building different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "class AlgoML:\n",
    "    \"\"\"Interface\"\"\"\n",
    "    def train_model(self, X_train, y_train) -> Any:\n",
    "        pass\n",
    "\n",
    "class AlgoXgboost(AlgoML):\n",
    "    def train_model(self, X_train, y_train) -> Any:\n",
    "        # shift + tab to unindent multiple lines \n",
    "        param_grid = {\n",
    "            'n_estimators': range(6, 10),\n",
    "            'max_depth' : range(3, 8),\n",
    "            'learning_rate' : [.2, .3, .4],\n",
    "            'colsample_bytree' : [.7, .8, .9, 1]\n",
    "        }\n",
    "\n",
    "        xgb = XGBClassifier()\n",
    "        # Searching for the best parameters\n",
    "        g_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=3, n_jobs=1, verbose=0, return_train_score=True)\n",
    "\n",
    "        # Fitting the model using best parameters found\n",
    "        g_search.fit(X_train, y_train)\n",
    "\n",
    "        # print the best parameters found\n",
    "        print(g_search.best_params_)\n",
    "        # valid \n",
    "        # g_search.score(X_valid, y_valid)\n",
    "        model = g_search\n",
    "        return model\n",
    "    \n",
    "class AlgoCustom(AlgoML):\n",
    "    def train_model(self, X_train, y_train) -> Any:\n",
    "        feature_position = ProbBinaryClassifier.feature_position(X_train, \"Sex_female\")\n",
    "        # config model\n",
    "        model = ProbBinaryClassifier(feature_position, 1)\n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "        return model\n",
    "    \n",
    "\n",
    "class AlgoFactory:\n",
    "    def get_algo(algo: str) -> AlgoML:\n",
    "        if algo == \"xgboost\":\n",
    "            return AlgoXgboost()\n",
    "        else:\n",
    "            return AlgoCustom()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traing a model \n",
    "\n",
    "* Xgboost model\n",
    "* Custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = AlgoFactory.get_algo(algo=ALGO)\n",
    "model = algo.train_model(X_train=X_train, y_train=y_train)\n",
    "predicts = model.predict(X_valid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator = ModelValidator(y_valid, predicts, dark_mode=DARK_MODE)\n",
    "scores_dict = validator.evaluate()\n",
    "validator.print_eval_result(scores_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 83 is false positive (false predicted to positive label, which has negative label as ground truth)\n",
    "validator.display_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC: Receiver operating characteristic\n",
    "# AUC -> 1\n",
    "validator.display_roc_curve()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model input feature attribution\n",
    "\n",
    "**local weighted regession surrogate explanation model**\n",
    "Kernel SHAP is based on LIME (local Interpretable Model-agnostic Explanations), which uses its own kernel to create a weighted linear regression surrogate surrounding the local data. \n",
    "\n",
    "Note:\n",
    "* The surrogate model is created for every instance x with sampled coalition. (https://christophm.github.io/interpretable-ml-book/shap.html), therefor the kernel SHAP is quite slow to execute.\n",
    "\n",
    "<!--\n",
    " ![local weighted regession surrogate explanation model](https://dl.acm.org/cms/attachment/ 382dae73-8228-4137-9aae-889784b9d7c5/f5.jpg) \n",
    "-->\n",
    "<img src=\"https://dl.acm.org/cms/attachment/382dae73-8228-4137-9aae-889784b9d7c5/f5.jpg\" width=\"700\" height=\"500\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Explainer callable for X_valid data set\n",
    "if KERNEL_EXPLAINER:\n",
    "    explainer = ModelKernelExplainer(model=model, train_data=X_train, inference_data=X_valid, dark_mode=DARK_MODE, link=LINK)\n",
    "else: \n",
    "    explainer = ModelExplainer(model=model, data=X_valid, dark_mode=DARK_MODE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ModelExplainer.valid_index\n",
    "def get_details(df: DataFrame, idx: int, org_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"get the passenger info from the original data frame based on the index position of validation set\n",
    "    @param df: \n",
    "    @param idx: index position, this must be idx so that decorator works\n",
    "    \"\"\"\n",
    "    # get the index name from the original raw dataset, from the index position of validation data set\n",
    "    index_name = df.iloc[idx].name\n",
    "    # use slicing on the same index name to get the passenger info as a DataFrame obj\n",
    "    return org_df.loc[index_name: index_name]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base values in SHAP\n",
    "Following the [SHAP paper](https://proceedings.neurips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf):\n",
    "\n",
    "the base value $E[f(z)]$ is \"the value that would be predicted if we did not know any features from the current output\". \n",
    "\n",
    "In other words, it is the mean of model prediction using our `background` or `reference` dataset: `X_train`\n",
    "```python3\n",
    "ModelKernelExplainer(model=model, train_data=X_train, ...)\n",
    "```\n",
    "the base value $E[f(z)]$:\n",
    "```python3\n",
    "base_value = np.mean(\n",
    "    model.predict_proba(X_train),\n",
    "    axis=0\n",
    ")\n",
    "```\n",
    "\n",
    "Note: \n",
    "* The kernelShap with `Link=\"identity\"` output the shap value in `probability` unit space\n",
    "* The kernelShap with `Link=\"logit\"` output the shap value in `log(odd)` unit space, which is more evenly distributed for loss and gains in a linear scale.\n",
    "\n",
    "Logit funciton $log_e(\\frac{p_x}{1 - p_x}) = log_e(p_x) - log_e(1 - p_x)$, `np.log()` uses Euler's number $e = 2.71828$ as base, which is the natural logarithm.\n",
    "\n",
    "Derivative of $e^x$ is $e^x$. \n",
    "In other words: $\\frac{d}{dx}(e^x) = e^x$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "def simple_logit(probs: Iterable):\n",
    "    # assumption all the probability in the Iterable shall be 1\n",
    "    return np.array([ np.log(x) - np.log(1-x) for x in probs])\n",
    "\n",
    "if getattr(explainer.explainer, \"expected_value\", None) is not None:\n",
    "    # print(explainer.explainer.expected_value)\n",
    "    print(f\"shap base value: \\n{explainer.explainer.expected_value}\")\n",
    "\n",
    "    # calculate base value for kernelshap with link identity and logit output space mapper\n",
    "    # average of all the local data predictions, X_train\n",
    "    all_local_data_predictions = model.predict_proba(X_train)\n",
    "    base_value = np.mean(all_local_data_predictions, axis=0)\n",
    "    if (LINK == 'logit'):\n",
    "        base_value = simple_logit(base_value)\n",
    "    print(f\"base value from local data predictions: \\n{base_value}\")\n",
    "else:\n",
    "    print(\"no expected_value in explainer\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examining the shap_values, as Explaination or List, Array from kernelExplainer\n",
    "# explainer.shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap.initjs()\n",
    "# shap.plots.force(explainer.explainer.expected_value[1], \n",
    "#                  explainer.shap_values[1], explainer.data, link=\"identity\")\n",
    "# shap.plots.force(explainer.explainer.expected_value[1], explainer.shap_values[1][0], explainer.data.iloc[0], link=\"identity\")\n",
    "# shap.plots.force(explainer.explainer.expected_value[0], explainer.shap_values[0][0], explainer.data.iloc[0], link=\"identity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pclass = 3.0 doesn't play any rules for our passenger at X_valid dataset with rank pos 0\n",
    "idx_pos = 0\n",
    "passager_df = get_details(df=X_valid, idx=idx_pos, org_df = train_X_y_raw_df)\n",
    "print(passager_df)\n",
    "explainer.force_plot(idx = idx_pos)\n",
    "explainer.waterfall_plot(idx = idx_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pclass = 3.0 doesn't play any rules for our passenger at X_valid dataset with rank pos 100\n",
    "idx_pos = 100\n",
    "passager_df = get_details(df=X_valid, idx=idx_pos, org_df = train_X_y_raw_df)\n",
    "print(passager_df)\n",
    "explainer.force_plot(idx = idx_pos)\n",
    "explainer.waterfall_plot(idx = idx_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_pos = 178\n",
    "# passager_df = get_details(df=X_valid, idx=idx_pos, org_df = train_X_raw_df)\n",
    "# print(passager_df)\n",
    "# explainer.force_plot(idx = idx_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_plot is a wrapper of beeswarm_plot(), kernel shap has no beeswarm_plot\n",
    "explainer.beeswarm_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.summary_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.scatter_plot(feature=\"Age\", auto_interact_features=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explainer.scatter_plot(feature=\"Age\", interact_feature=\"Sex_female\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.scatter_plot(feature=\"Pclass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.scatter_plot(feature=\"Fare\", interact_feature=\"Pclass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.scatter_plot(feature=\"Fare\", interact_feature=\"Sex_female\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play ground: Naiv gender based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Naiv gender based approach\"\"\"\n",
    "X_y_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# women = train_X_y_raw_df.loc[train_X_y_raw_df[\"Sex_female\"] == 1][\"Survived\"]\n",
    "# rate_women = sum(women) / len(women)\n",
    "\n",
    "def survived_rate(df: DataFrame, feature_name=\"Sex_female\", feature_value=1, label_name=\"Survived\"):\n",
    "    label_part = df.loc[df[feature_name] == feature_value][label_name]\n",
    "    return sum(label_part) / len(label_part)\n",
    "\n",
    "rate_women = survived_rate(X_y_train, \"Sex_female\")\n",
    "print(f\"% of women who survied: {rate_women:.2%}\")\n",
    "\n",
    "rate_men = survived_rate(X_y_train, \"Sex_male\")\n",
    "print(f\"% of men who survied: {rate_men:.2%}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play ground: kernel shap\n",
    "uncomment the following comments to play arround with shap.KernelExplainer to calculate the shap value estimate Shapley value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_kernel_explainer = shap.KernelExplainer(model=model.predict_proba, data=X_train.to_numpy(), link=LINK, algorithm=\"kernel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_shap_values = my_kernel_explainer.shap_values(X_valid.iloc[[0]].to_numpy(), nsamples=\"auto\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play ground: Data profiling for exploratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_y_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile = ProfileReport(all_X_raw_df, title=\"all raw data without filling the missing value report\")\n",
    "# profile = ProfileReport(X_y_train, title=\"all raw data without filling the missing value report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile.to_widgets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile.to_notebook_iframe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shap3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
