{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "import tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n",
    "from tf_agents.bandits.agents import dropout_thompson_sampling_agent as dropout_ts_agent\n",
    "from tf_agents.bandits.agents import lin_ucb_agent\n",
    "from tf_agents.bandits.agents import linear_thompson_sampling_agent as lin_ts_agent\n",
    "from tf_agents.bandits.agents import neural_epsilon_greedy_agent as eps_greedy_agent\n",
    "from tf_agents.bandits.agents.examples.v2 import trainer\n",
    "from tf_agents.bandits.environments import environment_utilities\n",
    "#from tf_agents.bandits.environments import movielens_per_arm_py_environment\n",
    "from tf_agents.bandits.environments import movielens_py_environment\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "from tf_agents.bandits.networks import global_and_arm_feature_network\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.policies import policy_saver\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import time_step as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target Directory if don't exist\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "fdate = date.today().strftime('%d_%m_%Y')\n",
    "\n",
    "root_path = os.getcwd() \n",
    "log_path = \"{}/logs/{}\".format(root_path, fdate)\n",
    "if not os.path.exists(log_path):\n",
    "    os.mkdir(log_path)\n",
    "    print(\"Directory {} Created\".format(fdate))\n",
    "else:    \n",
    "    print(\"Directory {} already exists\".format(fdate))\n",
    "\n",
    "print(\"Full path is {}\".format(log_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dowload movielens dataset from https://grouplens.org/datasets/movielens/\n",
    "# !wget https://files.grouplens.org/datasets/movielens/ml-25m.zip ./18rl/data/ml-25m.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the movielens pyenvironment with default parameters\n",
    "NUM_ACTIONS = 20 # take this as 20\n",
    "RANK_K = 20 # take rank as 20\n",
    "BATCH_SIZE = 8 # take batch size as 8\n",
    "data_path = \"./dataset/movielens.data\"\n",
    "# data_path = \"gs://ta-reinforecement-learning/dataset/movielens.data\" # specify the path to the movielens.data OR get it from the GCS bucket\n",
    "env = movielens_py_environment.MovieLensPyEnvironment(\n",
    "        data_path, RANK_K, BATCH_SIZE, num_movies=NUM_ACTIONS)\n",
    "environment = tf_py_environment.TFPyEnvironment(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 0.05\n",
    "LAYERS = (50, 50, 50)\n",
    "LR = 0.005\n",
    "DROPOUT_RATE = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Qnetwork\n",
    "network = q_network.QNetwork(\n",
    "          input_tensor_spec=environment.time_step_spec().observation,\n",
    "          action_spec=environment.action_spec(),\n",
    "          fc_layer_params=LAYERS)\n",
    "\n",
    "# Creating a neuron Epsilon greedy agent with an optimizer, \n",
    "# Epsilon exploration value, learning & dropout rate\n",
    "agent = eps_greedy_agent.NeuralEpsilonGreedyAgent(\n",
    "  time_step_spec=environment.time_step_spec(),# get the spec/format of the environment\n",
    "  action_spec=environment.action_spec(), # get the spec/format of the environment\n",
    "  reward_network=network, #q network goes here\n",
    "  # optimizer=tf.optimizers.Adam(learning_rate=LR),\n",
    "  optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=LR), #start w/ adam optimizer with a learning rate of .002\n",
    "  epsilon=EPSILON) # we recommend an exploration of value of 1%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making functions for computing optimal reward/action and attaching the env variable to it using partial functions, so it doesnt need to be passed with every invocation\n",
    "optimal_reward_fn = functools.partial(\n",
    "      environment_utilities.compute_optimal_reward_with_movielens_environment,\n",
    "      environment=environment)\n",
    "\n",
    "optimal_action_fn = functools.partial(\n",
    "      environment_utilities.compute_optimal_action_with_movielens_environment,\n",
    "      environment=environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initilializing the regret and suboptimal arms metric using the optimal reward and action functions\n",
    "regret_metric = tf_bandit_metrics.RegretMetric(optimal_reward_fn)\n",
    "suboptimal_arms_metric = tf_bandit_metrics.SuboptimalArmsMetric(\n",
    "      optimal_action_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_metric = tf_metrics.EnvironmentSteps()\n",
    "metrics = [tf_metrics.NumberOfEpisodes(),  #equivalent to number of steps in bandits problem\n",
    "           regret_metric,  # measures regret\n",
    "           suboptimal_arms_metric,  # number of times the suboptimal arms are pulled\n",
    "           tf_metrics.AverageReturnMetric(batch_size=environment.batch_size)  # the average return\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS_PER_LOOP = 2\n",
    "buf = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "      data_spec=agent.policy.trajectory_spec,\n",
    "      batch_size=BATCH_SIZE,\n",
    "      max_length=STEPS_PER_LOOP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOFINISH: setup the replay observer as a list to capture both metrics, step metrics and provide access to the function to load data from the driver into the buffer\n",
    "replay_observer = [buf.add_batch, step_metric] + metrics \n",
    "\n",
    "driver = dynamic_step_driver.DynamicStepDriver(\n",
    "      env=environment,\n",
    "      policy=agent.collect_policy,\n",
    "      num_steps=STEPS_PER_LOOP * environment.batch_size,\n",
    "      observers=replay_observer )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT_CHECKPOINT_NAME = 'agent'\n",
    "STEP_CHECKPOINT_NAME = 'step'\n",
    "CHECKPOINT_FILE_PREFIX = 'ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_and_get_checkpoint_manager(root_dir, agent, metrics, step_metric):\n",
    "    \"\"\"Restores from `root_dir` and returns a function that writes checkpoints.\"\"\"\n",
    "    trackable_objects = {metric.name: metric for metric in metrics}\n",
    "    trackable_objects[AGENT_CHECKPOINT_NAME] = agent\n",
    "    trackable_objects[STEP_CHECKPOINT_NAME] = step_metric\n",
    "    checkpoint = tf.train.Checkpoint(**trackable_objects)\n",
    "    checkpoint_manager = tf.train.CheckpointManager(checkpoint=checkpoint,\n",
    "                                                  directory=root_dir,\n",
    "                                                  max_to_keep=5)\n",
    "    latest = checkpoint_manager.latest_checkpoint\n",
    "\n",
    "    if latest is not None:\n",
    "        print('Restoring checkpoint from %s.', latest)\n",
    "        checkpoint.restore(latest)\n",
    "        print('Successfully restored to step %s.', step_metric.result())\n",
    "    else:\n",
    "        print('Did not find a pre-existing checkpoint. '\n",
    "                 'Starting from scratch.')\n",
    "    return checkpoint_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_manager = restore_and_get_checkpoint_manager(\n",
    "  log_path, agent, metrics, step_metric)\n",
    "saver = policy_saver.PolicySaver(agent.policy)\n",
    "summary_writer = tf.summary.create_file_writer(log_path)\n",
    "summary_writer.set_as_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT_ALPHA = 10.0\n",
    "TRAINING_LOOPS = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\"])\n",
    "# if tf.config.list_physical_devices('GPU'):\n",
    "#     strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\"], \n",
    "#                                               cross_device_ops=tf.distribute.NcclAllReduce)\n",
    "#     # strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "# else:\n",
    "#     strategy = tf.distribute.get_strategy()\n",
    "# print(strategy)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# with strategy.scope():\n",
    "for _ in range(0, 1):\n",
    "    for i in range(TRAINING_LOOPS):\n",
    "        driver.run()\n",
    "        batch_size = driver.env.batch_size\n",
    "        \n",
    "        dataset = buf.as_dataset( \n",
    "            sample_batch_size = BATCH_SIZE,\n",
    "            num_steps=STEPS_PER_LOOP,\n",
    "            single_deterministic_pass=True)\n",
    "\n",
    "        experience, unused_info = next(iter(dataset))\n",
    "        \n",
    "        train_loss = agent.train(experience).loss\n",
    "        buf.clear()\n",
    "        metric_utils.log_metrics(metrics)\n",
    "            # for m in metrics:\n",
    "            # print(m.name, \": \", m.result())\n",
    "        for metric in metrics:\n",
    "            metric.tf_summaries(train_step=step_metric.result())\n",
    "        checkpoint_manager.save()\n",
    "        if not i % 1000:  \n",
    "            print(f\"current {i // 1000}\")\n",
    "    \n",
    "    saver.save(os.path.join(log_path, \"./\", 'policy_%d' % step_metric.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tensorboard dev upload --logdir {} --name \\\"(optional) My latest experiment\\\" --description \\\"(optional) Agent trained\\\"\".format(log_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "feature = np.reshape(environment._observe()[0], (1,20))\n",
    "feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inference\n",
    "step = ts.TimeStep(\n",
    "        tf.constant(\n",
    "            ts.StepType.FIRST, dtype=tf.int32, shape=[1],\n",
    "            name='step_type'),\n",
    "        tf.constant(0.0, dtype=tf.float32, shape=[1], name='reward'),\n",
    "        tf.constant(1.0, dtype=tf.float32, shape=[1], name='discount'),\n",
    "        tf.constant(feature,\n",
    "                    dtype=tf.float64, shape=[1, 20],\n",
    "                    name='observation'))\n",
    "\n",
    "agent.policy.action(step).action.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfrl3.10",
   "language": "python",
   "name": "tfrl3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
